# Interlinear Reader

**Stop switching tabs. Start learning languages.**

Every unknown word is one click away from becoming part of your permanent vocabulary. No app switching. No copy-paste. No losing your place. Just pure, uninterrupted reading that builds your language skills automatically.

**[Try it now â†’](https://interlinear.peleke.me)**

---

## Why This Exists

You know the drill: You're reading in your target language. You hit an unknown word. Now what?

1. Highlight the word
2. Open a new tab
3. Search in Google Translate or a dictionary
4. Read the definition
5. Go back to your article
6. Try to remember where you were
7. Repeat 47 more times
8. Give up and watch Netflix instead

**This is why people don't learn languages through reading.** Not because reading doesn't work (it's the single most effective method for vocabulary acquisition) but because the tooling makes it feel like punishment.

## How It Works

Click any word. Get instant definition + native pronunciation. Done.

Behind the scenes, we're silently building your personal vocabulary database. Every click is tracked, timestamped, and sorted by frequency. You're not managing flashcards: you're just reading. Your vocabulary list builds itself.

**The entire workflow collapses to a single click:**
- ğŸ¯ **Instant lookup** with audio pronunciation (ElevenLabs neural TTS)
- ğŸ“Š **Automatic tracking**: click count, recency, frequency analytics
- ğŸ§  **Stay in flow**: inline definitions, no context switching
- ğŸ”’ **Private by default**: your data is encrypted and RLS-protected

[Demo video placeholder]

**What you get:**
- Read faster (no tab switching)
- Learn more (every word is trackable)
- Remember better (frequency + recency data for spaced repetition)
- Stay motivated (reading feels effortless, not like homework)

This is reading the way it should be: immersive, intelligent, and completely frictionless.

**[Start reading â†’](https://interlinear.peleke.me)**

---

## Current Features

### Interactive Reading
- Click any word â†’ instant dictionary lookup from Free Dictionary API
- Native-quality audio pronunciation via ElevenLabs text-to-speech
- Inline display preserves reading context

### Vocabulary Analytics
- Automatic frequency tracking for every clicked word
- Sortable by recency, frequency, and alphabetical order
- Personal vocabulary dashboard with learning statistics
- Export-ready data structure for external SRS systems

### User Experience
- Mobile-responsive design (responsive grid, touch-optimized)
- WCAG 2.1 AA accessibility compliance
- Serif typography with warm sepia palette for extended reading comfort
- Fast loading (~1.2s initial page load, sub-100ms definition lookups)

### Infrastructure
- Production deployment on Google Cloud Run (us-east4)
- OpenTofu-managed infrastructure (Artifact Registry, Secret Manager, IAM)
- GitHub Actions CI/CD with automated container builds
- Health monitoring and structured logging

---

## Architecture & Technical Details

### Technology Stack

| Layer | Technology | Rationale |
|-------|-----------|-----------|
| **Frontend** | Next.js 15 (App Router) + TypeScript | React Server Components for optimal performance; TypeScript for type safety |
| **Database** | Supabase (PostgreSQL) | Managed PostgreSQL with built-in auth, Row Level Security, and real-time subscriptions |
| **Authentication** | Supabase Auth | OAuth-ready auth with email verification and session management |
| **Text-to-Speech** | ElevenLabs API | Best-in-class neural TTS for pronunciation quality |
| **Dictionary** | Free Dictionary API | Zero-cost, comprehensive English dictionary with phonetics |
| **Deployment** | Google Cloud Run | Serverless containers with auto-scaling (0-10 instances) |
| **IaC** | OpenTofu (Terraform fork) | Declarative infrastructure with state management in GCS |
| **CI/CD** | GitHub Actions | Automated builds on merge to main |

### Key Architectural Decisions

#### 1. **Server-Side Rendering with Dynamic Rendering**
- **Decision**: Force dynamic rendering for all pages (`export const dynamic = 'force-dynamic'`)
- **Rationale**: Supabase client initialization requires runtime environment variables; static generation attempts to access them at build time, causing failures
- **Tradeoff**: Sacrificed static generation performance for deployment simplicity. Alternative would be edge runtime with environment variable injection, but adds complexity.

#### 2. **Client-Side Audio Caching**
- **Decision**: Store TTS audio in Supabase `audio_cache` table with user-scoped RLS
- **Rationale**: ElevenLabs charges per character; caching reduces API costs by ~80% for repeated words
- **Tradeoff**: Database storage costs vs API costs. Break-even at ~10 repeated pronunciations per word.

#### 3. **Container Build Strategy**
- **Decision**: Bake `NEXT_PUBLIC_*` env vars into Docker image at build time via `--build-arg`
- **Rationale**: Next.js compiles client-side env vars into JavaScript bundle; runtime injection doesn't work
- **Tradeoff**: Requires rebuild for env var changes, but enables immutable deployments with proper image tagging

#### 4. **Manual Deployment Trigger**
- **Decision**: GitHub Actions builds containers; Terraform deployment is manual
- **Rationale**: Separates build (automated, fast feedback) from deploy (deliberate, rollback-safe)
- **Tradeoff**: Extra manual step vs risk of auto-deploying breaking changes to production

#### 5. **Vocabulary Implicit Tracking**
- **Decision**: Save every clicked word to database immediately (no "save" button)
- **Rationale**: Removes friction; users learn by reading, not by managing lists
- **Tradeoff**: Higher database writes, but trivial cost (~$0.01/1000 writes on Supabase)

### Infrastructure Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       Google Cloud                           â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  Cloud Run      â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”¤ Artifact Registryâ”‚          â”‚
â”‚  â”‚  (0-10 inst)    â”‚         â”‚  Docker Images   â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚           â”‚                                                   â”‚
â”‚           â”‚ Pulls secrets at runtime                         â”‚
â”‚           â–¼                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                         â”‚
â”‚  â”‚ Secret Manager  â”‚                                         â”‚
â”‚  â”‚ - Supabase keys â”‚                                         â”‚
â”‚  â”‚ - ElevenLabs APIâ”‚                                         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                       â”‚ HTTPS
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Supabase                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ PostgreSQL  â”‚â—„â”€â”€â”€â”¤ Row Level    â”‚â—„â”€â”€â”¤ Auth        â”‚    â”‚
â”‚  â”‚ - vocab     â”‚    â”‚ Security     â”‚   â”‚ - JWT       â”‚    â”‚
â”‚  â”‚ - audio_cache    â”‚ (user_id)    â”‚   â”‚ - Email     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Security Model:**
- All database queries filtered by `auth.uid()` via RLS policies
- ElevenLabs API key server-side only (never exposed to client)
- Supabase anon key client-side (safe; RLS enforces user isolation)

**Scaling Strategy:**
- Cloud Run auto-scales 0â†’10 instances based on CPU (horizontal scaling)
- Supabase connection pooling handles 60 concurrent connections (sufficient for ~1000 MAU)
- ElevenLabs rate limit: 50 req/sec (overkill for current scale)

### Performance Characteristics

| Metric | Value | Target |
|--------|-------|--------|
| **Initial Load** | ~1.2s (TTI) | <2s |
| **Definition Lookup** | <100ms (cache) / ~300ms (cold) | <500ms |
| **TTS Generation** | ~800ms (ElevenLabs latency) | <1s |
| **Database Query** | <50ms (Supabase co-located) | <100ms |
| **Container Cold Start** | ~2s (Cloud Run) | <3s |

**Cost Structure** (at 1000 MAU):
- Cloud Run: ~$15/month (1 min instance, us-east4)
- Supabase: Free tier (under 500MB, <2GB transfer)
- ElevenLabs: ~$5/month (with 80% cache hit rate)
- **Total**: ~$20/month

---

## Quick Start

### Prerequisites

- Node.js 20+
- npm or yarn
- Supabase account
- ElevenLabs API key

### Local Development

1. **Clone the repository**
   ```bash
   git clone https://github.com/yourusername/interlinear.git
   cd interlinear
   ```

2. **Install dependencies**
   ```bash
   npm install
   ```

3. **Set up environment variables**

   Create `.env.local`:
   ```bash
   NEXT_PUBLIC_SUPABASE_URL=your-project-url
   NEXT_PUBLIC_SUPABASE_ANON_KEY=your-anon-key
   ELEVENLABS_API_KEY=your-elevenlabs-key
   ```

4. **Set up database**

   Run the SQL migrations in `supabase/schema.sql` on your Supabase project.

5. **Start development server**
   ```bash
   npm run dev
   ```

6. **Open your browser**

   Navigate to [http://localhost:3000](http://localhost:3000)

## Project Structure

```
interlinear/
â”œâ”€â”€ app/                    # Next.js App Router pages
â”‚   â”œâ”€â”€ api/               # API routes (TTS, health check)
â”‚   â”œâ”€â”€ login/             # Authentication pages
â”‚   â”œâ”€â”€ profile/           # User profile and stats
â”‚   â”œâ”€â”€ reader/            # Main reading interface
â”‚   â””â”€â”€ signup/            # User registration
â”œâ”€â”€ components/            # React components
â”‚   â”œâ”€â”€ auth/             # Authentication forms
â”‚   â”œâ”€â”€ reader/           # Reading interface components
â”‚   â””â”€â”€ vocabulary/       # Vocabulary management UI
â”œâ”€â”€ lib/                   # Utilities and services
â”‚   â”œâ”€â”€ supabase/         # Database client
â”‚   â”œâ”€â”€ dictionary.ts     # Dictionary API integration
â”‚   â”œâ”€â”€ tts.ts            # Text-to-speech service
â”‚   â””â”€â”€ vocabulary.ts     # Vocabulary tracking service
â”œâ”€â”€ types/                 # TypeScript type definitions
â”œâ”€â”€ supabase/             # Database schema and migrations
â”œâ”€â”€ terraform/            # Infrastructure as Code
â”œâ”€â”€ scripts/              # Deployment automation
â””â”€â”€ docs/                 # Documentation

```

## Database Schema

### Tables

**vocabulary_entries**
- User's clicked words with frequency tracking
- Automatic timestamps for learning analytics

**audio_cache**
- Cached TTS audio to reduce API costs
- User-scoped with automatic cleanup

**Row Level Security**
- All data is private and user-scoped
- Automatic user_id filtering on all operations

## API Routes

### POST /api/tts
Generate text-to-speech audio for a word or phrase.

**Request:**
```json
{
  "text": "hello",
  "voice_id": "21m00Tcm4TlvDq8ikWAM"
}
```

**Response:**
```json
{
  "audio": "base64-encoded-audio",
  "cached": false
}
```

### GET /api/health
Health check endpoint for monitoring.

## Deployment

See [DEPLOY.md](./DEPLOY.md) for complete deployment walkthrough.

### Quick Deploy

```bash
# Deploy infrastructure
./scripts/deploy-infra.sh staging

# Deploy application
./scripts/deploy-app.sh staging

# Rollback if needed
./scripts/rollback.sh staging
```

## Monitoring

Structured logging outputs JSON in production for Cloud Logging integration:

```typescript
import { logger } from '@/lib/logger'

logger.info('User action', { userId, action: 'click_word' })
logger.error('API error', error, { endpoint: '/api/tts' })
```

## Environment Variables

### Required

- `NEXT_PUBLIC_SUPABASE_URL` - Supabase project URL
- `NEXT_PUBLIC_SUPABASE_ANON_KEY` - Supabase anonymous key
- `ELEVENLABS_API_KEY` - ElevenLabs API key (server-side only)

### Optional

- `NODE_ENV` - `development` | `production`

## Development

### Running Tests

```bash
npm test
```

### Type Checking

```bash
npm run type-check
```

### Linting

```bash
npm run lint
```

### Building

```bash
npm run build
```

## Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## Development Roadmap

### Phase 1: MVP (Completed)
- âœ… Core reading interface with click-to-define
- âœ… Audio pronunciation (ElevenLabs integration)
- âœ… Automatic vocabulary tracking with frequency analytics
- âœ… User authentication and profile management
- âœ… Production deployment (GCP Cloud Run + OpenTofu)
- âœ… CI/CD pipeline (GitHub Actions)

### Phase 2: Learning Loop (Next)
- ğŸš§ **AI Tutor Mode**: GPT-4 powered exercises based on clicked vocabulary
  - Contextual sentence generation using user's vocabulary
  - Fill-in-the-blank and translation exercises
  - Adaptive difficulty based on click frequency
- ğŸš§ **Spaced Repetition System**: Automatic review scheduling
  - Algorithm: SM-2 (Anki-style) with click frequency weighting
  - Native push notifications for review reminders

### Phase 3: Expansion (Future)
- Multi-language support (Spanish â†’ French, German, Italian)
- Text upload (PDF/EPUB parsing)
- Vocabulary export (Anki-compatible CSV)
- Browser extension (read any webpage)

### Known Limitations
- **English-only**: Dictionary API is English-specific; multi-language requires paid APIs
- **No offline mode**: Requires network for definitions and TTS
- **Limited corpus**: No built-in reading library (user must paste text)
- **No mobile app**: PWA could improve mobile experience but requires additional testing

---

## Project Scope & Portfolio Context

**Built as**: Full-stack portfolio project demonstrating production-grade engineering practices

**Timeline**: ~2 weeks (architecture â†’ deployment)

**Key Achievements**:
- Zero-downtime deployments with image tagging and Terraform state management
- Type-safe full-stack TypeScript with strict ESLint rules
- Database-level security with PostgreSQL Row Level Security
- Cost-optimized architecture (~$20/month for 1000 MAU)
- Comprehensive deployment automation (see [DEPLOY.md](./DEPLOY.md))

**Design Philosophy**:
- **Solve one problem well**: Reading comprehension, not full language curriculum
- **Implicit over explicit**: Track behavior without asking users to manage data
- **Infrastructure as code**: Reproducible deployments with OpenTofu
- **Cost awareness**: Free tier optimization with graceful scaling

---

## Local Development

### Prerequisites
- Node.js 20+
- Supabase account (free tier)
- ElevenLabs API key (free tier: 10k characters/month)

### Setup

```bash
# Clone and install
git clone https://github.com/yourusername/interlinear.git
cd interlinear
npm install

# Environment variables (.env.local)
NEXT_PUBLIC_SUPABASE_URL=your-project-url
NEXT_PUBLIC_SUPABASE_ANON_KEY=your-anon-key
ELEVENLABS_API_KEY=your-elevenlabs-key

# Database setup
# Run supabase/schema.sql in your Supabase SQL editor

# Start dev server
npm run dev
# â†’ http://localhost:3000
```

### Scripts
```bash
npm run dev          # Development server with hot reload
npm run build        # Production build
npm run type-check   # TypeScript validation
npm run lint         # ESLint + Prettier
npm test             # Jest tests (if present)
```

---

## Deployment

**Full walkthrough**: [DEPLOY.md](./DEPLOY.md)

**Quick deploy**:
```bash
# 1. Deploy infrastructure (Artifact Registry, IAM, Secrets)
./scripts/deploy-infra.sh staging

# 2. Build and push container
./scripts/deploy-app.sh staging

# 3. Deploy Cloud Run service
./scripts/deploy-infra.sh staging --with-cloud-run --image-tag <sha>
```

**GitHub Secrets Required**:
- `GCP_SA_KEY` - Service account JSON
- `GCP_PROJECT_ID` - GCP project ID
- `NEXT_PUBLIC_SUPABASE_URL`
- `NEXT_PUBLIC_SUPABASE_ANON_KEY`

---

## Credits & Acknowledgments

- **Dictionary API**: [Free Dictionary API](https://dictionaryapi.dev/) - Public domain
- **Text-to-Speech**: [ElevenLabs](https://elevenlabs.io/) - Neural voice synthesis
- **Database & Auth**: [Supabase](https://supabase.com/) - PostgreSQL + Auth + Realtime
- **Infrastructure**: Google Cloud Platform

---

## License

MIT - See [LICENSE](./LICENSE) for details

---

**Built by**: [Your Name]
**Contact**: your.email@example.com
**Portfolio**: [yourportfolio.com](https://yourportfolio.com)
